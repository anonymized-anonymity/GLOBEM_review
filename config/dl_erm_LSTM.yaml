name: "dl_erm_LSTM"

feature_definition:
  use_norm_features: True

data_loader:
  batch_size: 512
  # Define how each data batch is sampled from multiple datasets and/or people
  # option: "across_dataset", "within_dataset", "across_person", "within_person"
  generate_by: "across_dataset" 
  
  mixup: null # option to mixup data: null, "across" (person or dataset), "within" (person or dataset)
  mixup_alpha: null # the strongness of mixup. from 0 to 1

model_params:
  arch: "LSTM" # model architecture
  # input dimension
  # 2: a multi-channel time series data (for models like 1dCNN)
  # 3: a single-channel image data (for models like 2dCNN)
  input_dim: 2
  lstm_shapes: [20,20] # a list of convolutional layers shapes
  embedding_size: 8 # vector length of the feature embedding network output
  flag_y_vector: True # True: one hot vector on y for regular training
  flag_bidirection: True # whether to use bidirectional arch
  flag_allsequences: True # whether to return all intermediate hidden states

training_params:
  optimizer: "Adam" # option: SGD or Adam
  learning_rate: 0.001
  epochs: 200
  steps_per_epoch: 100 # number of batches per epoch
  cos_annealing_step: 100
  cos_annealing_decay: 0.95
  best_epoch_strategy: "direct" # option: direct or on_test
  verbose: 0
  skip_training: False
